{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddharth17542/project-sid/blob/main/image_style_transfer_playground_with_gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zErexq1zv03b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import PIL\n",
        "import yaml\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from torchvision.models import vgg19\n",
        "from torchvision.utils import save_image\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Upload your content and style images by running the cell below:"
      ],
      "metadata": {
        "id": "wUxjXYoUDx93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "f5ohSlbgxMol",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b9da87d3-1e4e-42d5-bdc6-cbfe06e04437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-de140acc-43a8-445b-9f6b-e988194b6698\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-de140acc-43a8-445b-9f6b-e988194b6698\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving figures.jpg to figures.jpg\n",
            "Saving mosaic.jpg to mosaic.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Specify the file name of uploaded content image and style image:\n",
        "\n",
        "#@markdown Enter the content image file name.\n",
        "content_filename = \"figures.jpg\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Enter the style image file name.\n",
        "style_filename = \"mosaic.jpg\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "Qz5ahz7KD6mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Some general settings for user experience:\n",
        "\n",
        "#@markdown Check this to stop showing debugging messages, loss function values during training process, and stops generating intermediate images.\n",
        "quiet = True #@param {type:\"boolean\"}\n",
        "\n",
        "#@markdown Check this to automatically download output images after being generated.\n",
        "download = False #@param {type:\"boolean\"}"
      ],
      "metadata": {
        "id": "7SOQhxc1fWPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Here are some settings you could adjust for the output image (leave it blank to apply default values):\n",
        "\n",
        "#@markdown Size of the output image. Either one integer or two integers (height, weight) separated by comma is accepted. Will use the dimensions of content image if not provided.\n",
        "output_size = \"128\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown Format of the output image. Can be either \"jpg\", \"png\", \"jpeg\", or \"same\". If \"same\", output image will have the same format as the content image. \"jpg\" will be the default format.\n",
        "output_image_format = \"jpg\" #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "AbC1lgHHGiGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You may also provide a training configuration file in yaml format to set customized values for hyperparameters during the training process. May include the following hyperparameters:\n",
        "\n",
        "- num_epochs\n",
        "- learning_rate\n",
        "- alpha\n",
        "- beta\n",
        "- capture_content_features_from\n",
        "- capture_style_features_from\n",
        "\n",
        "Note that not providing a yaml file for configuration does not affect the program's functionality. An output image will be synthesized using default hyperparameter values. Run the following cell to upload your training configuration file (must be in .yaml):"
      ],
      "metadata": {
        "id": "TFk2JSejNGhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_config_uploaded = files.upload()\n",
        "train_config_filename = list(train_config_uploaded.keys())[0]\n",
        "\n",
        "print(\"Loading training configuration file...\")\n",
        "try:\n",
        "    with open(train_config_filename, 'r') as f:\n",
        "        training_configuration = yaml.safe_load(f)\n",
        "except FileNotFoundError:\n",
        "    print(f\"ERROR: could not find such file: '{train_config_filename}'.\")\n",
        "except yaml.YAMLError:\n",
        "    print(f\"ERROR: fail to load yaml file: '{train_config_filename}'.\")\n",
        "else:\n",
        "    print(\"Training configuration file successfully loaded.\")"
      ],
      "metadata": {
        "id": "e2V4yPqhNGJq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "6cc9956b-960e-4c90-a496-7f1234113c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e7790973-e487-45da-83ec-627075197620\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e7790973-e487-45da-83ec-627075197620\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving example_train_config.yaml to example_train_config (1).yaml\n",
            "Loading training configuration file...\n",
            "Training configuration file successfully loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(image_path, device, output_size=None, normalize=False):\n",
        "    \"\"\"Loads an image by transforming it into a tensor.\"\"\"\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    output_dim = None\n",
        "    if output_size is None:\n",
        "        output_dim = (img.size[1], img.size[0])\n",
        "    elif isinstance(output_size, int):\n",
        "        output_dim = (output_size, output_size)\n",
        "    elif isinstance(output_size, tuple):\n",
        "        if (len(output_size) == 2) and isinstance(output_size[0], int) and isinstance(output_size[1], int):\n",
        "            output_dim = output_size\n",
        "    else:\n",
        "        raise ValueError(\"ERROR: output_size must be an integer or a 2-tuple of (height, width) if provided.\")\n",
        "\n",
        "    torch_loader = transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(output_dim),\n",
        "            transforms.ToTensor()\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    img_tensor = torch_loader(img).unsqueeze(0)\n",
        "    return img_tensor.to(device)\n",
        "\n",
        "\n",
        "def get_image_name_ext(img_path):\n",
        "    \"\"\"Get name and extension of the image file from its path.\"\"\"\n",
        "    return os.path.splitext(os.path.basename(img_path))[0], os.path.splitext(os.path.basename(img_path))[1][1:]"
      ],
      "metadata": {
        "id": "nHvZjAAJxmIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageStyleTransfer_VGG19(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImageStyleTransfer_VGG19, self).__init__()\n",
        "\n",
        "        self.chosen_features = {0: 'conv11', 5: 'conv21', 10: 'conv31', 19: 'conv41', 28: 'conv51'}\n",
        "        self.model = vgg19(weights='DEFAULT').features[:29]\n",
        "\n",
        "    def forward(self, x):\n",
        "        feature_maps = dict()\n",
        "        for idx, layer in enumerate(self.model):\n",
        "            x = layer(x)\n",
        "            if idx in self.chosen_features.keys():\n",
        "                feature_maps[self.chosen_features[idx]] = x\n",
        "\n",
        "        return feature_maps\n",
        "\n",
        "\n",
        "def _get_content_loss(content_feature, generated_feature):\n",
        "    \"\"\"Compute MSE between content feature map and generated feature map as content loss.\"\"\"\n",
        "    return torch.mean((generated_feature - content_feature) ** 2)\n",
        "\n",
        "\n",
        "def _get_style_loss(style_feature, generated_feature):\n",
        "    \"\"\"Compute MSE between gram matrix of style feature map and of generated feature map as style loss.\"\"\"\n",
        "    _, channel, height, width = generated_feature.shape\n",
        "    style_gram = style_feature.view(channel, height*width).mm(\n",
        "        style_feature.view(channel, height*width).t()\n",
        "    )\n",
        "    generated_gram = generated_feature.view(channel, height*width).mm(\n",
        "        generated_feature.view(channel, height*width).t()\n",
        "    )\n",
        "\n",
        "    return torch.mean((generated_gram - style_gram) ** 2)\n",
        "\n",
        "\n",
        "def train(content, style, generated, device, train_config, output_dir, output_img_fmt, content_img_name, style_img_name, verbose=False):\n",
        "    \"\"\"Update the output image using pre-trained VGG19 model.\"\"\"\n",
        "    model = ImageStyleTransfer_VGG19().to(device).eval()    # freeze parameters in the model\n",
        "\n",
        "    # set default value for each configuration if not specified in train_config\n",
        "    num_epochs = train_config.get('num_epochs') if train_config.get('num_epochs') is not None else 6000\n",
        "    lr = train_config.get('learning_rate') if train_config.get('learning_rate') is not None else 0.001\n",
        "    alpha = train_config.get('alpha') if train_config.get('alpha') is not None else 1\n",
        "    beta = train_config.get('beta') if train_config.get('beta') is not None else 0.01\n",
        "    capture_content_features_from = train_config.get('capture_content_features_from') \\\n",
        "        if train_config.get('capture_content_features_from') is not None else {'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}\n",
        "    capture_style_features_from = train_config.get('capture_style_features_from') \\\n",
        "        if train_config.get('capture_style_features_from') is not None else {'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}\n",
        "\n",
        "    # check if values passed to capture_content_features_from and capture_style_features_from are valid\n",
        "    if not isinstance(capture_content_features_from, set):\n",
        "        if isinstance(capture_content_features_from, dict):\n",
        "            capture_content_features_from = set(capture_content_features_from.keys())\n",
        "        elif isinstance(capture_content_features_from, str):\n",
        "            capture_content_features_from = set([item.strip() for item in capture_content_features_from.split(',')])\n",
        "        else:\n",
        "            print(f\"ERROR: invalid value for 'capture_content_features_from' in training configuration file: {capture_content_features_from}.\")\n",
        "            return 0\n",
        "\n",
        "    if not capture_content_features_from.issubset({'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}):\n",
        "        print(f\"ERROR: invalid value for 'capture_content_features_from' in training configuration file: {capture_content_features_from}.\")\n",
        "        return 0\n",
        "\n",
        "    if not isinstance(capture_style_features_from, set):\n",
        "        if isinstance(capture_style_features_from, dict):\n",
        "            capture_style_features_from = set(capture_style_features_from.keys())\n",
        "        elif isinstance(capture_style_features_from, str):\n",
        "            capture_style_features_from = set([item.strip() for item in capture_style_features_from.split(',')])\n",
        "        else:\n",
        "            print(f\"ERROR: invalid value for 'capture_style_features_from' in training configuration file: {capture_style_features_from}.\")\n",
        "            return 0\n",
        "\n",
        "    if not capture_style_features_from.issubset({'conv11', 'conv21', 'conv31', 'conv41', 'conv51'}):\n",
        "        print(f\"ERROR: invalid value for 'capture_style_features_from' in training configuration file: {capture_style_features_from}.\")\n",
        "        return 0\n",
        "\n",
        "    optimizer = torch.optim.Adam([generated], lr=lr)\n",
        "\n",
        "    if verbose:\n",
        "        # create a directory to save intermediate results\n",
        "        intermediate_dir = os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-intermediate')\n",
        "        if not os.path.exists(intermediate_dir):\n",
        "            os.makedirs(intermediate_dir)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # get features maps of content, style and generated images from chosen layers\n",
        "        content_features = model(content)\n",
        "        style_features = model(style)\n",
        "        generated_features = model(generated)\n",
        "\n",
        "        content_loss = style_loss = 0\n",
        "\n",
        "        for layer_name in generated_features.keys():\n",
        "            content_feature = content_features[layer_name]\n",
        "            style_feature = style_features[layer_name]\n",
        "            generated_feature = generated_features[layer_name]\n",
        "\n",
        "            content_loss_per_feature = _get_content_loss(content_feature, generated_feature)\n",
        "            style_loss_per_feature = _get_style_loss(style_feature, generated_feature)\n",
        "\n",
        "            if layer_name in capture_content_features_from:\n",
        "                content_loss += content_loss_per_feature\n",
        "\n",
        "            if layer_name in capture_style_features_from:\n",
        "                style_loss += style_loss_per_feature\n",
        "\n",
        "        # compute loss\n",
        "        total_loss = alpha * content_loss + beta * style_loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print loss value and save progress every 200 epochs\n",
        "        if verbose:\n",
        "            if (epoch + 1) % 200 == 0:\n",
        "                save_image(generated, os.path.join(intermediate_dir, f'nst-{content_img_name}-{style_img_name}-{epoch + 1}.{output_img_fmt}'))\n",
        "\n",
        "                print(f\"\\tEpoch {epoch + 1}/{num_epochs}, loss = {total_loss.item()}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\t================================\")\n",
        "        print(f\"\\tIntermediate images are saved in directory: '{intermediate_dir}'\")\n",
        "        print(\"\\t================================\")\n",
        "\n",
        "    return 1\n"
      ],
      "metadata": {
        "id": "uPevYlVmyxzf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    image_dir = output_dir = '.'\n",
        "    content_path = os.path.join(image_dir, content_filename)\n",
        "    style_path = os.path.join(image_dir, style_filename)\n",
        "\n",
        "    verbose = not quiet\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Loading content and style images...\")\n",
        "\n",
        "    try:\n",
        "        content_img = Image.open(content_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: could not find such file: '{content_path}'.\")\n",
        "        return\n",
        "    except PIL.UnidentifiedImageError:\n",
        "        print(f\"ERROR: could not identify image file: '{content_path}'.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        style_img = Image.open(style_path)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"ERROR: could not find such file: '{style_path}'.\")\n",
        "        return\n",
        "    except PIL.UnidentifiedImageError:\n",
        "        print(f\"ERROR: could not identify image file: '{style_path}'.\")\n",
        "        return\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # load content and style images\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    output_dim = output_size.strip()\n",
        "    if len(output_dim) == 0:\n",
        "        output_dim = None\n",
        "    else:\n",
        "        try:\n",
        "            output_dim = [int(item.strip()) for item in output_dim.split(',')]\n",
        "        except ValueError:\n",
        "            print(f\"ERROR: invalid input for output_size: '{output_size}'. Should be integers separeted by comma.\")\n",
        "            return\n",
        "\n",
        "        if len(output_dim) > 1:\n",
        "            output_dim = tuple(output_dim)\n",
        "        else:\n",
        "            output_dim = output_dim[0]\n",
        "\n",
        "    content_tensor = load_image(content_path, device, output_size=output_dim)\n",
        "    output_size = (content_tensor.shape[2], content_tensor.shape[3])\n",
        "    style_tensor = load_image(style_path, device, output_size=output_dim)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Content and style images successfully loaded.\")\n",
        "        print()\n",
        "        print(\"Initializing output image...\")\n",
        "\n",
        "    # initialize output image\n",
        "    generated_tensor = content_tensor.clone().requires_grad_(True)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Output image successfully initialized.\")\n",
        "        print()\n",
        "\n",
        "    if 'training_configuration' not in globals():\n",
        "        train_config = dict()\n",
        "    else:\n",
        "        train_config = training_configuration.copy()\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Training...\")\n",
        "\n",
        "    content_img_name, content_img_fmt = get_image_name_ext(content_path)\n",
        "    style_img_name, _ = get_image_name_ext(style_path)\n",
        "\n",
        "    output_img_fmt = output_image_format.strip()\n",
        "    if len(output_img_fmt) == 0: output_img_fmt = 'jpg'\n",
        "    elif output_img_fmt == 'same': output_img_fmt = content_img_fmt\n",
        "    elif output_img_fmt not in {'jpg', 'png', 'jpeg', 'same'}:\n",
        "        print(f\"ERROR: invalid input for output_img_fmt: {output_img_fmt}. Should be one of \\\"jpg\\\", \\\"png\\\", \\\"jpeg\\\", \\\"same\\\".\")\n",
        "        return\n",
        "\n",
        "    # train model\n",
        "    success = train(content_tensor, style_tensor, generated_tensor, device, train_config, output_dir, output_img_fmt, content_img_name, style_img_name, verbose=verbose)\n",
        "\n",
        "    # save output image to specified directory\n",
        "    if success:\n",
        "        save_image(generated_tensor, os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-final.{output_img_fmt}'))\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"Output image successfully generated as {os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-final.{output_img_fmt}')}.\")\n",
        "\n",
        "    # download final output image\n",
        "    if download:\n",
        "        files.download(os.path.join(output_dir, f'nst-{content_img_name}-{style_img_name}-final.{output_img_fmt}'))"
      ],
      "metadata": {
        "id": "pXjXtKutwl_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "0RItSyPSwl82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 294
        },
        "outputId": "ee008f5a-5191-4e78-d40a-d7bf9845c0ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-263240bbee7e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-9865660c5cf8>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# load content and style images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'output_size' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nvr6gRQJWpJ_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}